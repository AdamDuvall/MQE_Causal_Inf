---
title: "MQE: Economic Inference from Data:  \nModule 3: Instrumental Variables"
author: "Claire Duquennois"
date: "6/9/2020"
output:
  beamer_presentation: default
  pdf_document: default
  slidy_presentation: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=9, fig.height=5) 
```

## You can't always get what you want

Even with fixed effects, certain types of unobservables can still bias our estimates. 

For OVB to not be a problem, we want a treatment variable $x_i$ where we know that there does not exist some omitted variable $x_{ov}$ such that

- $cor(x_i,x_{ov})\neq 0$ 

- and $cor(y_i, x_{ov})\neq 0$.

This is a tall order...

## You can't always get what you want

 But if you try \underline{sometimes},
 
 

## You can't always get what you want

 But if you try \underline{sometimes},
 
 you just \underline{might} find, 
 


## You can't always get what you want

 But if you try \underline{sometimes},
 
 you just \underline{might} find, 
 
 you get what you need: a good instrumental variable.
 
 
 
## An instrument for what? 

I am interested in the relationship between $y$ and $x_1$.

The true data generating process looks like this:

$$
y_i=\beta_0+\beta_1x_1+\beta_2x_2+\epsilon
$$

- $x_i$ and $x_2$ are uncorrelated with $\epsilon$ 

- $x_i$ and $x_2$ they are correlated with each other such that $Cov(x_1,x_2)\neq 0$

So whats the problem? 

- you don't actually observe $x_2$. 

Uh oh. 

## The problem: 

The naive approach (but you of course know better then to do this...) 

Regress $y$ on just $x_1$:

$$
y_i=\beta_0+\beta_1x_1+\nu
$$
where 

$$
\nu=\beta_2x_2+\epsilon.
$$

## The problem: 

$$
\begin{aligned}
\hat{\beta}_{1,OLS}&=\frac{cov(x_1,y)}{var(x_1)}\\
&=\frac{cov(x_1,\beta_0+\beta_1x_1+\nu)}{var(x_1)}\\
&=\frac{cov(x_1, \beta_0)+cov(x_1,\beta_1x_1)+cov(x_1,\nu)}{var(x_1)}\\
&=\frac{\beta_1var(x_1)+cov(x_1,\nu)}{var(x_1)}\\
&=\beta_1+\frac{cov(x_1,\nu)}{var(x_1)}
\end{aligned}
$$

 $cov(x_1,\nu)\neq0\Rightarrow\hat{\beta}_{1,OLS}$ is biased.

## All is not lost!
 

An **instrumental variable** (IV) is a variable that 

- is correlated with the "good" or "\textit{exogenous}" variation in $x_1$

- is unrelated to the "bad" or "\textit{endogenous}" or "\textit{related-to-$x_2$}" variation in $x_1$. 
 
 
## Formally


An IV is a variable, $z$ that satisfies two important properties:

- $Cov(z, x_1)\neq 0$ (the first stage).
- $Cov(z, \nu)= 0$ (the exclusion restriction). 

## The First Stage

$Cov(z, x_1)\neq 0$ 

- $z$ and $x_1$ are correlated

- the IV is useless without a first stage. 

We are trying to get a $\hat{\beta}_1$ such that $E[\hat{\beta}_1]=\beta_1$. If our instrument is totally unrelated to $x_1$, we won't have any hope of using it to get at $\beta_1$.

## The exclusion restriction

$Cov(z, \nu)= 0$ 

- $z$ has to affect $y$ **only** through $x_1$. 

- $\Rightarrow Cov(z,\epsilon)=0$ (because we've already assumed that $x_2$ is uncorrelated with $\epsilon$).


## The IV estimator
$$
\begin{aligned}
\hat{\beta}_{1,IV}&=\frac{cov(z,y)}{cov(z,x)}\\
&=\frac{cov(z,\beta_0+\beta_1x_1+\nu)}{cov(z,x_1)}\\
&=\beta_1\frac{cov(z,x_1)}{cov(z,x_1)}+\frac{cov(z,\nu)}{cov(z,x_1)}\\
&=\beta_1+\frac{cov(z,\nu)}{cov(z,x_1)}.
\end{aligned}
$$
 
With the exclusion restriction: $cov(z, \nu)= 0\Rightarrow E[\hat{\beta}_{1,IV}]=\beta_1$ 

Woot Woot! We have an unbiased estimator!

## Chasing Unicorns

- $z$'s that satisfy the first condition are easy to find, and we can test that $Cov(z, x_1)\neq 0$

- $z$'s that satisfy the exclusion restriction are rare and we cannot test that  $Cov(z, \nu)= 0$ since we don't observe $\epsilon$. 

## Chasing Unicorns

A good IV is not unlike a unicorn. It is quite powerful/magical as it will allow you to recover a consistent estimate of $\hat{\beta}_1$ in a situation that was otherwise hopeless.

\centering![]("images\real_unicorn.jpg"){width=75%}



## Chasing Unicorns

It is also a rare, (some may argue imaginary) beast, that usually turns out to be a horse with an overly optimistic rider (author).

\centering![]("images\unicorn.jpg"){width=45%}

\small
- be skeptical of instrumental variables regressions

- be wary of trying them yourself

- be prepared to convince people the exclusion restriction is satisfied


## A simulation

I generate some simulated data, with properties I fully understand:

The DGP: $Y$ depends on two variables, $X_1$ and $X_2$ such that 

$$
Y_i=\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}+\epsilon_i
$$

- $x_1$ and $x_2$ are correlated with $Cor(x_1,x_2)=0.75$

- $z$ is correlated with $x_1$ such that $Cor(x_1,z)=0.25$

- $z$ is not correlated with $x_2$ (so $Cor(x_2,z)=0$). 

## A simulation

\tiny
```{r simiv1, echo=TRUE}
library(MASS)
library(ggplot2)
library(stargazer)

sigmaMat<-matrix(c(1,0.75,0.25,0.75,1,0,0.25,0,1), nrow=3)
sigmaMat


set.seed(3221)
ivdat<- as.data.frame(mvrnorm(10000, mu = c(0,0,0), 
                     Sigma = sigmaMat))

names(ivdat)<-c("x_1","x_2","z")
cor(ivdat)
```

## A simulation

\tiny
```{r simiv1a, echo=TRUE}

ivdat$error<-rnorm(10000, mean=0, sd=1)

#The data generating process
B1<-10
B2<-(-20)

ivdat$Y<-ivdat$x_1*B1+ivdat$x_2*B2+ivdat$error

knitr::kable(head(ivdat))

```



## A simulation:
\small
```{r simiv 4,  echo=TRUE}
simiv1<-lm(Y~x_1+x_2, data=ivdat)
simiv2<-lm(Y~x_1, data=ivdat)
```
\normalsize
How will our estimate of $\hat{\beta}_1$ in model 2 compare to the true $\beta$? 

$\Rightarrow$ Top Hat

## A simulation:
\tiny
```{r simiv 4a, results = "asis", echo=TRUE}
stargazer(simiv1, simiv2, header=FALSE, type='latex', omit.stat = "all", single.row = TRUE)
```

\small
- With the correctly specified model $E[\hat{\beta}_1]=\beta_1$.

- If I do not observe $x_2$, the naive approach is biased.

## A simulation:


Suppose there exists a variable $z$ that satisfies the two conditions outlined above: 

- $Cov(z, V_1)\neq 0$ (the first stage).

- $Cov(z, \nu)= 0$ (the exclusion restriction). 

Our simulated data includes $z$, a variable with these properties 
\tiny
```{r simiv 5, results = "asis", echo=TRUE}
cor(ivdat$z, ivdat$x_1)

#note: we can test this correlation because I am working with simulated data and observe x_2.
#In the wild x_2 would be unobservable and you would have to argue that this condition holds.

ivdat$nu<-B2*ivdat$x_2+ivdat$error
cor(ivdat$z, ivdat$nu)
```

## A simulation:

I instrument my endogenous variable, $x_1$, with my instrument $z$:

\tiny
```{r simiv 6, results = "asis", echo=TRUE}
library(lfe)

simiv3<-felm(Y~1|0|(x_1~z),ivdat)

```


## A simulation:


:::::: {.columns}

::: {.column width="30%" data-latex="{0.55\textwidth}"}

![]("images\dab_uni.jpg")

\small
- I get an unbiased estimate of  $\beta_1$!

- Careful: $R^2$ values get real funky (negative!?!)-- don't use.

:::


::: {.column width="70%" data-latex="{0.4\textwidth}"}

\tiny
```{r simiv 6a, results = "asis", echo=TRUE}
stargazer(simiv1, simiv2, simiv3, header=FALSE, 
          type='latex', omit.stat = c("n", "f","ser"))
```

:::
::::::

## 2SLS:

How does $\beta_{IV}$ uses the instrumental variable to retrieve an unbiased estimate?

To build intuition, let's look at the two-stage least squares (2SLS) estimator $\beta_{2SLS}$.

When we are working with only one instrument and one endogenous regressor, $\beta_{IV}=\beta_{2SLS}$.
